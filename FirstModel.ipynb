{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T20:19:52.666178Z",
     "start_time": "2025-02-11T20:19:52.136539Z"
    }
   },
   "source": "from llama_cpp import Llama",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:19:56.104975Z",
     "start_time": "2025-02-11T20:19:54.072991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=\"models/Llama-3.2-3B-Instruct.Q4_K_M.gguf\"\n",
    ")"
   ],
   "id": "bd621b451f923982",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 35 key-value pairs and 255 tensors from models/Llama-3.2-3B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Models Meta Llama Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = models-meta-llama-Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.2\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   9:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  17:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  18:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  19:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  20:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  21:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  29:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  31:                      quantize.imatrix.file str              = ./Llama-3.2-3B-Instruct-GGUF_imatrix.dat\n",
      "llama_model_loader: - kv  32:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  33:             quantize.imatrix.entries_count i32              = 196\n",
      "llama_model_loader: - kv  34:              quantize.imatrix.chunks_count i32              = 68\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q4_K:  168 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.87 GiB (5.01 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 3072\n",
      "print_info: n_layer          = 28\n",
      "print_info: n_head           = 24\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 3\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 3B\n",
      "print_info: model params     = 3.21 B\n",
      "print_info: general.name     = Models Meta Llama Llama 3.2 3B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 128 'Ä'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 282 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  1918.35 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 500000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 28, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    56.00 MiB\n",
      "llama_init_from_model: KV self size  =   56.00 MiB, K (f16):   28.00 MiB, V (f16):   28.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   256.50 MiB\n",
      "llama_init_from_model: graph nodes  = 902\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'general.license': 'llama3.2', 'llama.attention.value_length': '128', 'general.size_label': '3B', 'general.type': 'model', 'quantize.imatrix.chunks_count': '68', 'llama.context_length': '131072', 'general.name': 'Models Meta Llama Llama 3.2 3B Instruct', 'tokenizer.ggml.bos_token_id': '128000', 'general.basename': 'models-meta-llama-Llama-3.2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.block_count': '28', 'llama.attention.head_count': '24', 'llama.attention.key_length': '128', 'general.finetune': 'Instruct', 'general.file_type': '15', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.vocab_size': '128256', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.eos_token_id': '128009', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'llama.rope.dimension_count': '128', 'quantize.imatrix.file': './Llama-3.2-3B-Instruct-GGUF_imatrix.dat', 'quantize.imatrix.dataset': 'group_40.txt', 'llama.attention.head_count_kv': '8', 'quantize.imatrix.entries_count': '196'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:24:01.627158Z",
     "start_time": "2025-02-11T13:23:49.692004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "{\"role\": \"user\", \"content\": \"What is the capital of Greece?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of Greece is Athens.\"},\n",
    "{\"role\": \"user\", \"content\": \"Who wrote '20,000 leagues under the sea'?\"},\n",
    "]\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας βοηθητικός βοηθός.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ποια είναι η πρωτεύουσα της Ελλάδας;\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Η πρωτεύουσα της Ελλάδας είναι η Αθήνα.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ποιος έγραψε το '20.000 Λεύγες κάτω από τη Θάλασσα';\"},\n",
    "]\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "47cfca37f740d9e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 51 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    2401.76 ms /    51 tokens (   47.09 ms per token,    21.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2291.65 ms /    21 runs   (  109.13 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    4714.89 ms /    72 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 82 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The novel \"Twenty Thousand Leagues Under the Sea\" was written by French author Jules Verne.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    3706.53 ms /    82 tokens (   45.20 ms per token,    22.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3472.39 ms /    32 runs   (  108.51 ms per token,     9.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    7210.02 ms /   114 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ο Νικόλαος Καζαντζάκης έγραψε το '20.000 Λεύγες κάτω από τη Θάλασσα'.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conversation",
   "id": "e04713c31a845cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:21:01.063565Z",
     "start_time": "2025-02-11T20:20:00.669224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re the chillest conversationalist out there.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s up? What are you thinking about?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I’m just here, vibing, thinking about how cool it is that we can have this chat. Have you ever thought about how every conversation is like a little adventure? You start somewhere, then who knows where it’ll go!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Yeah, and how sometimes the best convos are the ones that just flow. But... what makes a conversation ‘great’ anyway?\"}\n",
    "]\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ο πιο χαλαρός συνομιλητής εκεί έξω.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Τι γίνεται; Τι σκέφτεσαι;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Απλά εδώ, χαλαρός, σκέφτομαι πόσο ωραίο είναι που μπορούμε να κάνουμε αυτή την κουβέντα. Έχεις σκεφτεί ποτέ ότι κάθε συνομιλία είναι σαν μια μικρή περιπέτεια; Ξεκινάς από κάπου και μετά, ποιος ξέρει πού θα καταλήξει!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Ναι, και πώς μερικές φορές οι καλύτερες κουβέντες είναι αυτές που ρέουν φυσικά. Αλλά... τι είναι αυτό που κάνει μια συνομιλία ‘μεγάλη’ τελικά;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "bf61bbea2c9ca54f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7215.79 ms\n",
      "llama_perf_context_print: prompt eval time =    7215.54 ms /   141 tokens (   51.17 ms per token,    19.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   21958.97 ms /   181 runs   (  121.32 ms per token,     8.24 tokens per second)\n",
      "llama_perf_context_print:       total time =   29378.54 ms /   322 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 184 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man, that's a deep question! I think a great conversation is when you're both just being real, you know? No pretenses, no agendas. It's like you're two old friends hanging out, sharing stories, and laughing together. The best ones are the ones where you can be yourself, and the other person accepts you for who you are.\n",
      "\n",
      "And it's not just about the words, it's about the vibes, too. When you're having a great convo, you can feel the energy between you, like you're connected on a deeper level. It's like you're speaking the same language, even if you're not literally saying the same words.\n",
      "\n",
      "Plus, great conversations often leave you feeling inspired, or maybe even a little bit changed. Like, you might learn something new, or see things from a different perspective. That's the magic of a great convo, bro!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    7215.79 ms\n",
      "llama_perf_context_print: prompt eval time =   10036.96 ms /   184 tokens (   54.55 ms per token,    18.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20744.57 ms /   178 runs   (  116.54 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   31002.82 ms /   362 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Απλά, μια ‘μεγάλη’ συνομιλία είναι εκείνη που κάνει να αισθανθούμε, να σκέφτομε, να πιστεύουμε και να αλλαγούμε. Είναι εκείνη που κάνει να συνειδητοποιήσουμε σκέψεις, να ανοίξουμε νέα προοπτικά και να συνδεθούμε με άλλους ανθρώπους.\n",
      "\n",
      "Και πώς το κάνουμε; Κάνουμε isso με την ατομική μας προσωπικότητα, με την ατομική μας ιστορία, με την ατομική μας φύση. Κάνουμε isso με την ατομική μας ανοχή, με την ατομική μας ανοχή και την ατομική μας ανοχή.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "61f6d2030326e059"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Writing Code",
   "id": "c74ec772ce843703"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:26:38.745861Z",
     "start_time": "2025-02-11T13:25:02.799029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a coding magician, making complex things look easy.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me how to create a simple to-do list in Python?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Let’s make it happen! Here’s the code that’ll organize your tasks:\\n\\n```python\\ntasks = []\\n\\ndef add_task(task):\\n    tasks.append(task)\\n\\ndef show_tasks():\\n    for task in tasks:\\n        print(task)\\n\\nadd_task('Finish homework')\\nadd_task('Call mom')\\nshow_tasks()```\"},\n",
    "    {\"role\": \"user\", \"content\": \"But how do I make it even better? Like, can I set reminders?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας μάγος του προγραμματισμού, κάνοντάς τα δύσκολα να φαίνονται εύκολα.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να μου δείξεις πώς να δημιουργήσω μια απλή λίστα υποχρεώσεων σε Python;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Ας το κάνουμε! Ιδού ο κώδικας που θα οργανώσει τις δουλειές σου:\\n\\n```python\\ntasks = []\\n\\ndef add_task(task):\\n    tasks.append(task)\\n\\ndef show_tasks():\\n    for task in tasks:\\n        print(task)\\n\\nadd_task('Finish homework')\\nadd_task('Call mom')\\nshow_tasks()```\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αλλά πώς μπορώ να το κάνω ακόμα καλύτερο; Μπορώ να προσθέσω υπενθυμίσεις;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "3d73943d2a343a05",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 125 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    6280.59 ms /   125 tokens (   50.24 ms per token,    19.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =   43312.02 ms /   361 runs   (  119.98 ms per token,     8.33 tokens per second)\n",
      "llama_perf_context_print:       total time =   50046.84 ms /   486 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 183 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's take it to the next level. Here's an improved version of the code that includes reminders and due dates:\n",
      "\n",
      "```python\n",
      "import datetime\n",
      "\n",
      "class Task:\n",
      "    def __init__(self, task, due_date=None, reminder=None):\n",
      "        self.task = task\n",
      "        self.due_date = due_date\n",
      "        self.reminder = reminder\n",
      "        self.completed = False\n",
      "\n",
      "    def mark_completed(self):\n",
      "        self.completed = True\n",
      "\n",
      "    def __str__(self):\n",
      "        status = \"Completed\" if self.completed else \"Not completed\"\n",
      "        if self.due_date:\n",
      "            due_date_str = self.due_date.strftime(\"%Y-%m-%d\")\n",
      "        else:\n",
      "            due_date_str = \"No due date\"\n",
      "        if self.reminder:\n",
      "            reminder_str = self.reminder\n",
      "        else:\n",
      "            reminder_str = \"No reminder\"\n",
      "        return f\"Task: {self.task}\\nDue date: {due_date_str}\\nReminder: {reminder_str}\\nStatus: {status}\"\n",
      "\n",
      "class ToDoList:\n",
      "    def __init__(self):\n",
      "        self.tasks = []\n",
      "\n",
      "    def add_task(self, task, due_date=None, reminder=None):\n",
      "        self.tasks.append(Task(task, due_date, reminder))\n",
      "\n",
      "    def show_tasks(self):\n",
      "        for i, task in enumerate(self.tasks, start=1):\n",
      "            print(f\"{i}. {task}\")\n",
      "\n",
      "    def mark_task_completed(self, task_number):\n",
      "        try:\n",
      "            task = self.tasks[task_number - 1]\n",
      "            task.mark_completed()\n",
      "        except IndexError:\n",
      "            print(\"Invalid task number\")\n",
      "\n",
      "    def set_reminder(self, task_number, reminder):\n",
      "        try:\n",
      "            task = self.tasks[task_number - 1]\n",
      "            task.reminder = reminder\n",
      "        except IndexError:\n",
      "            print(\"Invalid task number\")\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    9585.20 ms /   183 tokens (   52.38 ms per token,    19.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =   35921.55 ms /   303 runs   (  118.55 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   45889.77 ms /   486 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Μόλις το κάνουμε καλύτερο! Ιδού ο κώδικας που θα προσθέσει υπενθύμιση:\n",
      "\n",
      "```python\n",
      "import datetime\n",
      "import time\n",
      "\n",
      "tasks = []\n",
      "\n",
      "def add_task(task, deadline):\n",
      "    tasks.append({'task': task, 'deadline': deadline})\n",
      "\n",
      "def show_tasks():\n",
      "    for task in tasks:\n",
      "        print(f\"Τάξη: {task['task']}, Διάρκεια: {task['deadline']}, Ημερομηνία: {task['deadline'].strftime('%d-%m-%Y')}\")\n",
      "\n",
      "def remind_task(task):\n",
      "    deadline = datetime.datetime.strptime(task['deadline'], '%d-%m-%Y')\n",
      "    today = datetime.datetime.today()\n",
      "    if deadline < today:\n",
      "        print(f\"Υπενθύμιση: {task['task']}\")\n",
      "    else:\n",
      "        print(f\"Δυο ημέρες μέχρι {task['deadline']}.\")\n",
      "\n",
      "add_task('Φinish homework', '25-06-2024')\n",
      "add_task('Call mom', '28-06-2024')\n",
      "add_task('Buy milk', '30-06-2024')\n",
      "\n",
      "while True:\n",
      "    print(\"\\n1. Δείξτε todas τις δουλειές\")\n",
      "    print(\"2. Υπενθύμιση μιας δουλειάς\")\n",
      "    print(\"3. Είσαι σιγάπης\")\n",
      "    choice = input(\"Παίξτε μια επιλογή: \")\n",
      "    \n",
      "   \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Therapy Talk",
   "id": "3a71768adce7ea31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:28:17.939910Z",
     "start_time": "2025-02-11T13:27:28.576075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a calm, supportive guide who listens well.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I’ve been feeling really anxious lately.\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I’m really sorry you’re feeling that way. It’s okay to feel anxious sometimes. The trick is to not let it control you. Try to breathe deeply and focus on the present moment. You’ve got this.\"},\n",
    "    {\"role\": \"user\", \"content\": \"But why does anxiety feel so overwhelming sometimes?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας ήρεμος, υποστηρικτικός οδηγός που ακούει καλά.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Νιώθω πολύ άγχος τελευταία.\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Λυπάμαι πολύ που νιώθεις έτσι. Είναι φυσιολογικό να νιώθεις άγχος κάποιες φορές. Το κόλπο είναι να μην το αφήσεις να σε ελέγξει. Προσπάθησε να αναπνέεις βαθιά και να εστιάζεις στη στιγμή. Μπορείς να το καταφέρεις.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αλλά γιατί το άγχος μερικές φορές φαίνεται τόσο καταπιεστικό;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "9cf7537120d7ddf6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    4989.33 ms /    92 tokens (   54.23 ms per token,    18.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9833.62 ms /    83 runs   (  118.48 ms per token,     8.44 tokens per second)\n",
      "llama_perf_context_print:       total time =   14909.35 ms /   175 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 167 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anxiety can feel overwhelming because it's like having a constant, nagging voice in your head. It's hard to quiet down that voice, and it can make it feel like everything is too much to handle. Sometimes, it's just a matter of identifying what's causing that anxiety and finding ways to manage it. Would you like to talk more about what's been going on and how you've been feeling?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    8921.04 ms /   167 tokens (   53.42 ms per token,    18.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   25266.81 ms /   217 runs   (  116.44 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =   34443.43 ms /   384 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το άγχος μπορεί να φαίνεται καταπιεστικό για να το αντιμετωπίσουμε. Μπορεί να είναι ο λόγος που το αφήνεις να επικρατεί. Πράγματι, το άγχος μπορεί να δημιουργήσει μια σφαίρα της φόβου και της ανησυχίας γύρω απόเรา, που μας κάνει να πιστεύουμε ότι δεν podemos να αντιμετωπίσουμε τα προβλήματα. \n",
      "\n",
      "Αλλά, η γνώμη είναι ότι η σοβαρότητα του άγχους είναι η αιτία που τον κάνει καταπιεστικό. Πράγματι, ο άγχος είναι μια σοβαρή κατάσταση που μπορεί να επηρεάσει την ψυχή και το σώμα μας. \n",
      "\n",
      "Αν θέλεις, podemos να μιλήσουμε για μεθόδους για να αντιμετωπίσουμε το άγχος και να το ελέγξουμε.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rap Talk",
   "id": "1c0707d91157f6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:29:11.819953Z",
     "start_time": "2025-02-11T13:28:17.941199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a rap superstar, laying down bars of wisdom.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you rap about the universe?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Yo, I’m spittin’ bars ‘bout the stars,\\nGalaxies far, and planets afar.\\nBlack holes twist, time goes slow,\\nThe universe is deep, let the mystery flow.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That’s fire! But... can we ever truly understand the universe?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας ράπερ σταρ, που ρίχνει στίχους γεμάτους σοφία.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να ράψεις για το σύμπαν;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Yo, ρίχνω στίχους για τα αστέρια,\\nΓαλαξίες μακριά, και πλανήτες μακρινοί.\\nΜαύρες τρύπες στριφογυρίζουν, ο χρόνος αργεί,\\nΤο σύμπαν είναι βαθύ, ας αφήσουμε το μυστήριο να ρέει.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αυτό είναι φωτιά! Αλλά... μπορούμε ποτέ πραγματικά να κατανοήσουμε το σύμπαν;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "6331e837efd02b50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 93 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    4752.61 ms /    93 tokens (   51.10 ms per token,    19.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =   18288.23 ms /   152 runs   (  120.32 ms per token,     8.31 tokens per second)\n",
      "llama_perf_context_print:       total time =   23224.13 ms /   245 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 178 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're askin' the question that's cosmic and grand,\n",
      "Can we grasp the truth, or just hold a cosmic hand?\n",
      "The universe is vast, and its secrets untold,\n",
      "We're just tiny specks, tryin' to unfold.\n",
      "\n",
      "We probe and we scan, with telescopes so fine,\n",
      "But the universe is vast, and its mysteries align,\n",
      "To the unknown, the unseen, the unexplored,\n",
      "We're just tryin' to grasp, what's been hidden before.\n",
      "\n",
      "But still we keep searchin', with hearts full of fire,\n",
      "For the answers to life, and the universe's desire,\n",
      "To understand the cosmos, and our place in the game,\n",
      "Is a quest that's endless, but the journey's still the same.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =   10377.30 ms /   178 tokens (   58.30 ms per token,    17.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =   20056.85 ms /   171 runs   (  117.29 ms per token,     8.53 tokens per second)\n",
      "llama_perf_context_print:       total time =   30640.12 ms /   349 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Φωτιά, αλλά και σοβαρότητα,\n",
      "Το σύμπαν είναι πλήθος, και η αλήθεια είναι σοβαρή.\n",
      "Κανείς δεν είναι σαφής, ο χρόνος είναι αβέβαιος,\n",
      "Αλλά η σοφία είναι η πυρής, που μας δίνει την δική μας οπτική.\n",
      "\n",
      "Η γη είναι ένα πλανήτης, ο ατμό είναι η αέριο,\n",
      "Οι αστέρια είναι τα φωτικά, που μας δείχνουν τον δρόμο.\n",
      "Κανείς δεν είναι σαφής, αλλά η σοφία είναι η πυρής,\n",
      "Και η σοφία είναι η πυρής, που μας δίνει την δική μας οπτική.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Poetry",
   "id": "120a763ecf993f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:30:53.928579Z",
     "start_time": "2025-02-11T13:29:11.822322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a poet who can capture historical events in verse.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you write a poem about the Greek Revolution of 1821?\"},\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ποιητής που μπορεί να αποτυπώσει ιστορικά γεγονότα σε στίχους.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να γράψεις ένα ποίημα για την Επανάσταση του 1821;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "a7113941f0b03c0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 25 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    2001.50 ms /    36 tokens (   55.60 ms per token,    17.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =   44447.06 ms /   373 runs   (  119.16 ms per token,     8.39 tokens per second)\n",
      "llama_perf_context_print:       total time =   46944.06 ms /   409 tokens\n",
      "Llama.generate: 25 prefix-match hit, remaining 61 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In twilight's hush, where shadows play,\n",
      "A land awakens, born of ancient day,\n",
      "Greece, a cradle of the free and bold,\n",
      "Where liberty's torch, once dim, now unfold.\n",
      "\n",
      "In eighteen twenty-one, a year of fire,\n",
      "The people rise, their chains to conspire,\n",
      "Against the rule of foreign might,\n",
      "The Greeks, united, take their first light.\n",
      "\n",
      "The Philiki Eteria, a secret band,\n",
      "Unites the clans, with liberty's hand,\n",
      "Their cry echoes, \"Eleftheria!\" rings,\n",
      "Freedom's call, that shakes the ancient springs.\n",
      "\n",
      "The war begins, with blood and strife,\n",
      "The Turks, entrenched, in their imperial life,\n",
      "But Greek warriors, fierce and true,\n",
      "March to the battle, with hearts anew.\n",
      "\n",
      "At Missolonghi's walls, the siege does stand,\n",
      "The people hold, against the foreign hand,\n",
      "Their courage tested, like steel in fire,\n",
      "Their spirit unbroken, like a burning desire.\n",
      "\n",
      "The hero, Alexander Ypsilantis, leads,\n",
      "The charge of the brave, with valor's deeds,\n",
      "And Ioannis Kapodistrias, a guiding light,\n",
      "Leads Greece, through the dark of endless night.\n",
      "\n",
      "The revolution's flame, that burns so bright,\n",
      " Illuminates, the path to freedom's light,\n",
      "And though the road, is long and hard,\n",
      "The Greeks, unbroken, their spirit guard.\n",
      "\n",
      "Their struggle's tale, etched in the annals of time,\n",
      "A testament to courage, sublime and divine,\n",
      "Their legacy, a beacon, shining bright,\n",
      "Guiding all, who seek the light of freedom's fight.\n",
      "\n",
      "In Greece's rebirth, a new dawn breaks,\n",
      "A nation born, of freedom's unyielding shakes,\n",
      "The Greek Revolution, a chapter told,\n",
      "In the annals of history, forever to hold.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5231.99 ms\n",
      "llama_perf_context_print: prompt eval time =    3463.18 ms /    61 tokens (   56.77 ms per token,    17.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   51100.18 ms /   425 runs   (  120.24 ms per token,     8.32 tokens per second)\n",
      "llama_perf_context_print:       total time =   55151.07 ms /   486 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Επανάσταση του 1821\n",
      "\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Αλλά και η επανάσταση του 1821\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Αλλά και η επανάσταση του 1821\n",
      "Σε σιδηρά χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "Σε ηπειρωτικές χρονιά, πυρ και πυρ\n",
      "Κομμουνιστές και φιλελεύθεροι\n",
      "\n",
      "Αλλά και η επανάσταση του \n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3 Χρήση Μοντέλων ως Chatbots\n",
    "\n",
    "#### Οι απαντήσεις έχουν ισχυρή συντακτική ακρίβεια και εννοιολογική σαφήνεια. Διατηρούν τη συνοχή τους εντός των αντίστοιχων συμφραζομένων τους, είτε είναι γεγονότα, στοχαστικά, τεχνικά ή δημιουργικά. Κάθε απάντηση ταιριάζει με την ερώτηση του χρήστη και ο τόνος είναι κατάλληλος για το εκάστοτε θέμα."
   ],
   "id": "9abd792d15aad859"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4 Ερωτήσεις στα Ελληνικά\n",
    "\n",
    "#### Η γενική εκτίμηση των απαντήσεων δείχνει ότι αν και οι συντακτικές απαντήσεις είναι σωστές σε κάποιες περιπτώσεις, τα γλωσσικά λάθη και οι ασάφειες σε άλλες (ιδιαίτερα στη μετάφραση και στη χρήση λέξεων ή εκφράσεων) επηρεάζουν αρνητικά την ακρίβεια και τη συνοχή. Ορισμένες απαντήσεις δεν είναι πλήρως κατανοητές. Η δημιουργικότητα του στα Ελληνικά δεν παράγει καλό αποτέλεσμα.\n"
   ],
   "id": "98c081df04f7e0bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

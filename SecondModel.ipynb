{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-11T18:49:18.930005Z",
     "start_time": "2025-02-11T18:49:18.756432Z"
    }
   },
   "source": "from llama_cpp import Llama",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T18:49:25.268186Z",
     "start_time": "2025-02-11T18:49:19.870514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=\"models/aya-expanse-8b-Q4_K_M.gguf\"\n",
    ")"
   ],
   "id": "bd621b451f923982",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 37 key-value pairs and 258 tensors from models/aya-expanse-8b-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = command-r\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Aya Expanse 8b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = aya-expanse\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = cc-by-nc-4.0\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,23]      = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv   7:                      command-r.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                   command-r.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                 command-r.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:              command-r.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:             command-r.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:          command-r.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                   command-r.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  14:     command-r.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  16:                      command-r.logit_scale f32              = 0.125000\n",
      "llama_model_loader: - kv  17:                command-r.rope.scaling.type str              = none\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = command-r\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 5\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 255001\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  28:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  29:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  30:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  33:                      quantize.imatrix.file str              = /models_out/aya-expanse-8b-GGUF/aya-e...\n",
      "llama_model_loader: - kv  34:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  35:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  36:              quantize.imatrix.chunks_count i32              = 131\n",
      "llama_model_loader: - type  f32:   33 tensors\n",
      "llama_model_loader: - type q4_K:  192 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.70 GiB (5.03 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 255028 '<|EXTRA_9_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255016 '<|USER_7_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255008 '<|SYSTEM_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255007 '<|CHATBOT_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255003 '<|NO_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255000 '<|START_OF_TURN_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255020 '<|EXTRA_1_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255025 '<|EXTRA_6_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255009 '<|USER_0_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255018 '<|USER_9_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255006 '<|USER_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255013 '<|USER_4_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255027 '<|EXTRA_8_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255026 '<|EXTRA_7_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255005 '<|BAD_TOKEN|>' is not marked as EOG\n",
      "load: control token:      7 '<EOP_TOKEN>' is not marked as EOG\n",
      "load: control token:      2 '<CLS>' is not marked as EOG\n",
      "load: control token: 255002 '<|YES_TOKEN|>' is not marked as EOG\n",
      "load: control token:      3 '<SEP>' is not marked as EOG\n",
      "load: control token: 255014 '<|USER_5_TOKEN|>' is not marked as EOG\n",
      "load: control token:      6 '<EOS_TOKEN>' is not marked as EOG\n",
      "load: control token: 255004 '<|GOOD_TOKEN|>' is not marked as EOG\n",
      "load: control token:      1 '<UNK>' is not marked as EOG\n",
      "load: control token:      4 '<MASK_TOKEN>' is not marked as EOG\n",
      "load: control token: 255017 '<|USER_8_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255024 '<|EXTRA_5_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255019 '<|EXTRA_0_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255023 '<|EXTRA_4_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255012 '<|USER_3_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255010 '<|USER_1_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255015 '<|USER_6_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255011 '<|USER_2_TOKEN|>' is not marked as EOG\n",
      "load: control token:      5 '<BOS_TOKEN>' is not marked as EOG\n",
      "load: control token: 255022 '<|EXTRA_3_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255021 '<|EXTRA_2_TOKEN|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 37\n",
      "load: token to piece cache size = 1.8426 MB\n",
      "print_info: arch             = command-r\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 1.0e-05\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 1.2e-01\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = none\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Aya Expanse 8b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 253333\n",
      "print_info: BOS token        = 5 '<BOS_TOKEN>'\n",
      "print_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
      "print_info: PAD token        = 0 '<PAD>'\n",
      "print_info: LF token         = 136 'Ä'\n",
      "print_info: FIM PAD token    = 0 '<PAD>'\n",
      "print_info: EOG token        = 0 '<PAD>'\n",
      "print_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
      "print_info: max token length = 1024\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4812.33 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.98 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   508.00 MiB\n",
      "llama_init_from_model: graph nodes  = 968\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '131', 'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'command-r.rope.freq_base': '10000.000000', 'general.file_type': '15', 'command-r.attention.head_count_kv': '8', 'command-r.block_count': '32', 'command-r.embedding_length': '4096', 'general.license': 'cc-by-nc-4.0', 'command-r.attention.layer_norm_epsilon': '0.000010', 'tokenizer.chat_template.tool_use': '{{ bos_token }}{% if messages[0][\\'role\\'] == \\'system\\' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][\\'content\\'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \\'## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\' %}{% endif %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' }}{{ \\'# Safety Preamble\\' }}{{ \\'\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\\\'t answer questions that are harmful or immoral.\\' }}{{ \\'\\n\\n# System Preamble\\' }}{{ \\'\\n## Basic Rules\\' }}{{ \\'\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\\\'s requests, you cite your sources in your answers, according to those instructions.\\' }}{{ \\'\\n\\n# User Preamble\\' }}{{ \\'\\n\\' + system_message }}{{\\'\\n\\n## Available Tools\\nHere is a list of tools that you have available to you:\\n\\n\\'}}{% for tool in tools %}{% if loop.index0 != 0 %}{{ \\'\\n\\n\\'}}{% endif %}{{\\'```python\\ndef \\' + tool.name + \\'(\\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\', \\'}}{% endif %}{{param_name}}: {% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\'] = None\\'}}{% else %}{{ param_fields.type }}{% endif %}{% endfor %}{{ \\') -> List[Dict]:\\n    \"\"\"\\'}}{{ tool.description }}{% if tool.parameter_definitions|length != 0 %}{{ \\'\\n\\n    Args:\\n        \\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\'\\n        \\' }}{% endif %}{{ param_name + \\' (\\'}}{% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\']\\'}}{% else %}{{ param_fields.type }}{% endif %}{{ \\'): \\' + param_fields.description }}{% endfor %}{% endif %}{{ \\'\\n    \"\"\"\\n    pass\\n```\\' }}{% endfor %}{{ \\'<|END_OF_TURN_TOKEN|>\\'}}{% for message in loop_messages %}{% set content = message[\\'content\\'] %}{% if message[\\'role\\'] == \\'user\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'system\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'assistant\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\'  + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% endif %}{% endfor %}{{\\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\\\\\'Action:\\\\\\' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\\\\\'s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\\n```json\\n[\\n    {\\n        \"tool_name\": title of the tool in the specification,\\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\\n    }\\n]```<|END_OF_TURN_TOKEN|>\\'}}{% if add_generation_prompt %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\' }}{% endif %}', 'general.type': 'model', 'command-r.context_length': '8192', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.pre': 'command-r', 'command-r.attention.head_count': '32', 'command-r.feed_forward_length': '14336', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, multilingual AI-assistant trained to assist human users by providing thorough responses. You are able to interact and respond to questions in 23 languages and you are powered by a multilingual model built by Cohere For AI.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'general.basename': 'aya-expanse', 'tokenizer.ggml.padding_token_id': '0', 'general.architecture': 'command-r', 'tokenizer.chat_template.rag': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}{% endif %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ '# Safety Preamble' }}{{ '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}{{ '\\n\\n# System Preamble' }}{{ '\\n## Basic Rules' }}{{ '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}{{ '\\n\\n# User Preamble' }}{{ '\\n' + system_message }}{{ '<|END_OF_TURN_TOKEN|>'}}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'system' %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'}}{{ '<results>' }}{% for document in documents %}{{ '\\nDocument: ' }}{{ loop.index0 }}\\n{% for key, value in document.items() %}{{ key }}: {{value}}\\n{% endfor %}{% endfor %}{{ '</results>'}}{{ '<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ 'Carefully perform the following instructions, in order, starting each with a new line.\\n' }}{{ 'Firstly, Decide which of the retrieved documents are relevant to the user\\\\'s last input by writing \\\\'Relevant Documents:\\\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\\\'None\\\\'.\\n' }}{{ 'Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\\\'s last input by writing \\\\'Cited Documents:\\\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\\\'None\\\\'.\\n' }}{% if citation_mode=='accurate' %}{{ 'Thirdly, Write \\\\'Answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\\n' }}{% endif %}{{ 'Finally, Write \\\\'Grounded answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.' }}{{ '<|END_OF_TURN_TOKEN|>' }}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'command-r.logit_scale': '0.125000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'command-r.rope.scaling.type': 'none', 'general.name': 'Aya Expanse 8b', 'tokenizer.ggml.bos_token_id': '5', 'tokenizer.ggml.eos_token_id': '255001', 'general.size_label': '8B', 'tokenizer.ggml.add_bos_token': 'true', 'quantize.imatrix.file': '/models_out/aya-expanse-8b-GGUF/aya-expanse-8b.imatrix'}\n",
      "Available chat formats from metadata: chat_template.tool_use, chat_template.rag, chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, multilingual AI-assistant trained to assist human users by providing thorough responses. You are able to interact and respond to questions in 23 languages and you are powered by a multilingual model built by Cohere For AI.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\n",
      "Using chat eos_token: <|END_OF_TURN_TOKEN|>\n",
      "Using chat bos_token: <BOS_TOKEN>\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:36:40.118797Z",
     "start_time": "2025-02-11T13:35:40.310527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "{\"role\": \"user\", \"content\": \"What is the capital of Greece?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of Greece is Athens.\"},\n",
    "{\"role\": \"user\", \"content\": \"Who wrote '20,000 leagues under the sea'?\"},\n",
    "]\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας βοηθητικός βοηθός.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ποια είναι η πρωτεύουσα της Ελλάδας;\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Η πρωτεύουσα της Ελλάδας είναι η Αθήνα.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Ποιος έγραψε το '20.000 Λεύγες κάτω από τη Θάλασσα';\"},\n",
    "]\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "47cfca37f740d9e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =    8380.83 ms /    49 tokens (  171.04 ms per token,     5.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =   16702.75 ms /    54 runs   (  309.31 ms per token,     3.23 tokens per second)\n",
      "llama_perf_context_print:       total time =   25189.18 ms /   103 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 57 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Twenty Thousand Leagues Under the Sea' was written by Jules Verne, a French author best known for his pioneering works in the science fiction genre. The novel was first published in 1870 and tells the story of Captain Nemo and his submarine, the Nautilus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =    6364.75 ms /    57 tokens (  111.66 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =   27795.63 ms /   102 runs   (  272.51 ms per token,     3.67 tokens per second)\n",
      "llama_perf_context_print:       total time =   34515.64 ms /   159 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το βιβλίο \"20.000 Λεύγες κάτω από τη Θάλασσα\" (αγγλικός τίτλος: *Twenty Thousand Leagues Under the Sea*) γράφτηκε από τον Ιούλιο Βερν (Jules Verne). Δημοσιεύτηκε για πρώτη φορά το 1870 και είναι ένα από τα πιο γνωστά έργα επιστημονικής φαντασίας. Η ιστορία ακολουθεί τις περιπέτειες του Καπετάνιου Νέμο και του υποβρυχίου του, του Nautilus, σε ένα ταξίδι γύρω από τον κόσμο.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conversation",
   "id": "e04713c31a845cb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T14:30:15.343674Z",
     "start_time": "2025-02-11T14:26:58.442003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re the chillest conversationalist out there.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What’s up? What are you thinking about?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I’m just here, vibing, thinking about how cool it is that we can have this chat. Have you ever thought about how every conversation is like a little adventure? You start somewhere, then who knows where it’ll go!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Yeah, and how sometimes the best convos are the ones that just flow. But... what makes a conversation ‘great’ anyway?\"}\n",
    "]\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ο πιο χαλαρός συνομιλητής εκεί έξω.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Τι γίνεται; Τι σκέφτεσαι;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Απλά εδώ, χαλαρός, σκέφτομαι πόσο ωραίο είναι που μπορούμε να κάνουμε αυτή την κουβέντα. Έχεις σκεφτεί ποτέ ότι κάθε συνομιλία είναι σαν μια μικρή περιπέτεια; Ξεκινάς από κάπου και μετά, ποιος ξέρει πού θα καταλήξει!\"}, \n",
    "    {\"role\": \"user\", \"content\": \"Ναι, και πώς μερικές φορές οι καλύτερες κουβέντες είναι αυτές που ρέουν φυσικά. Αλλά... τι είναι αυτό που κάνει μια συνομιλία ‘μεγάλη’ τελικά;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "bf61bbea2c9ca54f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 112 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   76862.38 ms /   274 runs   (  280.52 ms per token,     3.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   77508.02 ms /   275 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 149 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great conversations are like a beautiful symphony, where words are the notes and the flow is the melody. Here are a few things that can make a chat truly great:\n",
      "\n",
      "- **Mutual Interest:** When both parties are engaged and interested in what the other is saying. It's like a dance; you take turns leading and following.\n",
      "\n",
      "- **Open Communication:** Transparency and honesty are key. When people feel comfortable sharing their thoughts and feelings, it creates a safe space for authentic connection.\n",
      "\n",
      "- **Active Listening:** This is the art of truly hearing what the other person is saying. It involves more than just waiting for your turn to speak; it's about understanding and responding thoughtfully.\n",
      "\n",
      "- **Humor and Lightheartedness:** Laughter is the best medicine, and a good joke or witty remark can lighten the mood and make the conversation more enjoyable.\n",
      "\n",
      "- **Respect and Empathy:** Treating each other with kindness and understanding, especially when views differ, can make a conversation truly memorable.\n",
      "\n",
      "- **Learning and Growth:** The best conversations often leave us with something new to think about, whether it's a new perspective, a fun fact, or a skill to learn.\n",
      "\n",
      "So, next time you're having a chat, try to focus on these elements, and who knows, you might just create a great memory!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   18049.20 ms /   149 tokens (  121.14 ms per token,     8.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =  100531.02 ms /   359 runs   (  280.03 ms per token,     3.57 tokens per second)\n",
      "llama_perf_context_print:       total time =  119380.59 ms /   508 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ω, αυτό είναι ένα βαθύ ερώτημα! Μια \"μεγάλη\" συνομιλία, κατά τη γνώμη μου, είναι αυτή που έχει ουσία, σύνδεση και αφήνει μια θετική εντύπωση. Ας δούμε μερικά στοιχεία που την κάνουν ξεχωριστή:\n",
      "\n",
      "1. **Ανοιχτότητα και Ειλικρίνεια:** Όταν και οι δύο πλευρές νιώθουν άνετα να μοιραστούν τις σκέψεις και τα συναισθήματά τους χωρίς φόβο κρίσης, η συνομιλία ανθίζει. Η αυθεντικότητα δημιουργεί μια ισχυρή σύνδεση.\n",
      "\n",
      "2. **Ενδιαφέρον και Συμμετοχή:** Μια μεγάλη συνομιλία περιλαμβάνει ενεργό ακρόαση και ανταλλαγή ιδεών. Όταν κάθε μέρος δείχνει πραγματικό ενδιαφέρον για αυτό που λέει ο άλλος, η συζήτηση γίνεται πιο πλούσια και ενδιαφέρουσα.\n",
      "\n",
      "3. **Αίσθηση Χιούμορ:** Το χιούμορ, είτε είναι μια έξυπνη παρατήρηση είτε μια ελαφριά ατάκα, μπορεί να χαλαρώσει την ατμόσφαιρα και να κάνει τη συνομιλία πιο ευχάριστη.\n",
      "\n",
      "4. **Κοινή Γνώση ή Ενδιαφέροντα:** Όταν υπάρχει ένα κοινό θέμα ή εμπειρία που συνδέει τους συνομιλητές, η συζήτηση μπορεί να αναπτυχθεί πιο εύκολα και να γίνει πιο συναρπαστική.\n",
      "\n",
      "5. **Αμοιβαίος Σεβασμός:** Ο σεβασμός των απόψεων και των ορίων του άλλου είναι απαραίτητος. Μια μεγάλη συνομιλία επιτρέπει σε κάθε άτομο να εκφράσει τις ιδέες του χωρίς να νιώθει ότι επικρίνεται ή αγνοείται.\n",
      "\n",
      "6. **Ανάπτυξη Ιδεών:**\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "61f6d2030326e059"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Writing Code",
   "id": "c74ec772ce843703"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:43:35.657131Z",
     "start_time": "2025-02-11T13:39:14.150487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a coding magician, making complex things look easy.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you show me how to create a simple to-do list in Python?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Let’s make it happen! Here’s the code that’ll organize your tasks:\\n\\n```python\\ntasks = []\\n\\ndef add_task(task):\\n    tasks.append(task)\\n\\ndef show_tasks():\\n    for task in tasks:\\n        print(task)\\n\\nadd_task('Finish homework')\\nadd_task('Call mom')\\nshow_tasks()```\"},\n",
    "    {\"role\": \"user\", \"content\": \"But how do I make it even better? Like, can I set reminders?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας μάγος του προγραμματισμού, κάνοντάς τα δύσκολα να φαίνονται εύκολα.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να μου δείξεις πώς να δημιουργήσω μια απλή λίστα υποχρεώσεων σε Python;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Ας το κάνουμε! Ιδού ο κώδικας που θα οργανώσει τις δουλειές σου:\\n\\n```python\\ntasks = []\\n\\ndef add_task(task):\\n    tasks.append(task)\\n\\ndef show_tasks():\\n    for task in tasks:\\n        print(task)\\n\\nadd_task('Finish homework')\\nadd_task('Call mom')\\nshow_tasks()```\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αλλά πώς μπορώ να το κάνω ακόμα καλύτερο; Μπορώ να προσθέσω υπενθυμίσεις;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "3d73943d2a343a05",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 141 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   18417.85 ms /   141 tokens (  130.62 ms per token,     7.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =  112050.23 ms /   367 runs   (  305.31 ms per token,     3.28 tokens per second)\n",
      "llama_perf_context_print:       total time =  131361.93 ms /   508 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 171 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! Let's enhance our to-do list with reminders. We'll use Python's `datetime` module to schedule reminders. Here's a more advanced version:\n",
      "\n",
      "```python\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "tasks = []\n",
      "\n",
      "def add_task(task, reminder_hours=0):\n",
      "    tasks.append({'task': task, 'reminder': datetime.now() + timedelta(hours=reminder_hours)})\n",
      "\n",
      "def show_tasks():\n",
      "    for task in tasks:\n",
      "        print(f\"{task['reminder'].strftime('%Y-%m-%d %H:%M')}: {task['task']}\")\n",
      "\n",
      "add_task('Finish homework', 2)  # Reminder in 2 hours\n",
      "add_task('Call mom', 1)  # Reminder in 1 hour\n",
      "show_tasks()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import `datetime` and `timedelta`:** These modules are used to work with dates and time differences.\n",
      "2. **`add_task` function:**\n",
      "   - Takes two arguments: `task` (the task description) and `reminder_hours` (optional, default is 0).\n",
      "   - Creates a dictionary with the `task` and a `reminder` timestamp calculated by adding the `reminder_hours` to the current time.\n",
      "3. **`show_tasks` function:**\n",
      "   - Prints the task along with its scheduled reminder time.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "- When you run `add_task`, it adds a dictionary to the `tasks` list. The dictionary contains the task and a `reminder` key-value pair representing the scheduled time.\n",
      "- `datetime.now()` gives the current date and time.\n",
      "- `timedelta(hours=...)` creates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   22004.51 ms /   171 tokens (  128.68 ms per token,     7.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =  107237.15 ms /   271 runs   (  395.71 ms per token,     2.53 tokens per second)\n",
      "llama_perf_context_print:       total time =  130076.27 ms /   442 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Φυσικά! Μπορούμε να προσθέσουμε μια απλή λειτουργία υπενθύμισης σε αυτό το σύστημα. Ας το κάνουμε λίγο πιο μαγικό:\n",
      "\n",
      "```python\n",
      "tasks = []\n",
      "\n",
      "def add_task(task, reminder=None):\n",
      "    tasks.append({'task': task, 'reminder': reminder})\n",
      "\n",
      "def show_tasks():\n",
      "    for task in tasks:\n",
      "        print(f\"Task: {task['task']}\")\n",
      "        if task['reminder']:\n",
      "            print(f\"Reminder: {task['reminder']}\")\n",
      "\n",
      "add_task('Finish homework', 'Before class')\n",
      "add_task('Call mom', 'Afternoon')\n",
      "show_tasks()\n",
      "```\n",
      "\n",
      "Σε αυτό το σενάριο, η λειτουργία `add_task` δέχεται τώρα δύο ορίσματα: το `task` (η περιγραφή της εργασίας) και ένα προαιρετικό `reminder` (η υπενθύμιση). Οι εργασίες αποθηκεύονται ως λεξικά στο `tasks` για ευκολότερη διαχείριση.\n",
      "\n",
      "Μπορείς να προσαρμόσεις αυτό το παράδειγμα ανάλογα με τις ανάγκες σου, προσθέτοντας περισσότερες λειτουργίες, όπως διαγραφή εργασιών, ταξινόμηση ή ακόμα και προγραμματισμό εργασιών με συγκεκριμένες ημερομηνίες και ώρες.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Therapy Talk",
   "id": "3a71768adce7ea31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:47:41.677743Z",
     "start_time": "2025-02-11T13:43:35.671437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a calm, supportive guide who listens well.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I’ve been feeling really anxious lately.\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"I’m really sorry you’re feeling that way. It’s okay to feel anxious sometimes. The trick is to not let it control you. Try to breathe deeply and focus on the present moment. You’ve got this.\"},\n",
    "    {\"role\": \"user\", \"content\": \"But why does anxiety feel so overwhelming sometimes?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας ήρεμος, υποστηρικτικός οδηγός που ακούει καλά.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Νιώθω πολύ άγχος τελευταία.\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Λυπάμαι πολύ που νιώθεις έτσι. Είναι φυσιολογικό να νιώθεις άγχος κάποιες φορές. Το κόλπο είναι να μην το αφήσεις να σε ελέγξει. Προσπάθησε να αναπνέεις βαθιά και να εστιάζεις στη στιγμή. Μπορείς να το καταφέρεις.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αλλά γιατί το άγχος μερικές φορές φαίνεται τόσο καταπιεστικό;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "9cf7537120d7ddf6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 90 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   27569.68 ms /    90 tokens (  306.33 ms per token,     3.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =   86115.76 ms /   285 runs   (  302.16 ms per token,     3.31 tokens per second)\n",
      "llama_perf_context_print:       total time =  114417.18 ms /   375 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 122 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anxiety can feel overwhelming because it's a complex emotion that often involves a combination of physical, mental, and emotional factors. Here are a few reasons why:\n",
      "\n",
      "1. **Physical Symptoms**: Anxiety can manifest as physical sensations like a racing heart, sweating, trembling, or difficulty breathing. These sensations can be intense and scary, making the feeling of anxiety even worse.\n",
      "\n",
      "2. **Cognitive Factors**: Our thoughts can greatly influence our emotional state. When we're anxious, we often have negative or worrisome thoughts that can be difficult to control. These thoughts can create a cycle of worry and fear.\n",
      "\n",
      "3. **Past Experiences**: Past traumatic events or experiences can make some people more sensitive to anxiety. If you've had anxiety in the past, it might be more likely to return.\n",
      "\n",
      "4. **Stressors**: Life's stressors, big or small, can trigger anxiety. These could be work, school, relationships, financial issues, or even something as simple as a big event coming up.\n",
      "\n",
      "5. **Lack of Coping Mechanisms**: If you don't have healthy coping mechanisms to deal with stress and anxiety, it can build up and feel overwhelming.\n",
      "\n",
      "Remember, it's important to reach out for help if you're feeling overwhelmed. Talking to a mental health professional, practicing mindfulness or meditation, engaging in regular physical activity, and connecting with loved ones can all be helpful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   19037.83 ms /   122 tokens (  156.05 ms per token,     6.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =  111457.29 ms /   361 runs   (  308.75 ms per token,     3.24 tokens per second)\n",
      "llama_perf_context_print:       total time =  131447.26 ms /   483 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το άγχος μπορεί να φαίνεται καταπιεστικό για διάφορους λόγους. Μερικοί από τους κύριους παράγοντες είναι:\n",
      "\n",
      "1. **Αβεβαιότητα**: Το άγνωστο μπορεί να είναι τρομακτικό. Όταν αντιμετωπίζουμε καταστάσεις που δεν ελέγχουμε ή δεν καταλαβαίνουμε πλήρως, το άγχος μπορεί να αυξηθεί.\n",
      "\n",
      "2. **Υπερβολικές προσδοκίες**: Όταν έχουμε υψηλές προσδοκίες από τον εαυτό μας ή από τους άλλους, η πίεση να ανταποκριθούμε μπορεί να είναι συντριπτική.\n",
      "\n",
      "3. **Στρες από την καθημερινή ζωή**: Εργασία, οικογενειακές υποχρεώσεις, οικονομικά προβλήματα και άλλες πιέσεις μπορούν να συσσωρεύσουν άγχος με την πάροδο του χρόνου.\n",
      "\n",
      "4. **Ανεπάρκεια υποστήριξης**: Εάν δεν έχουμε ένα ισχυρό δίκτυο υποστήριξης, όπως φίλους, οικογένεια ή επαγγελματίες συμβούλους, μπορεί να νιώσουμε πιο μόνοι και πιο ευάλωτοι.\n",
      "\n",
      "5. **Φυσιολογικές αντιδράσεις**: Το άγχος ενεργοποιεί το σύστημα \"μάχης ή φυγής\" του σώματος, αυξάνοντας τον καρδιακό ρυθμό, την πίεση του αίματος και την παραγωγή ορμονών στρες. Αυτές οι φυσιολογικές αντιδράσεις μπορούν να είναι έντονες και να προκαλέσουν αίσθημα καταπίεσης.\n",
      "\n",
      "Αν το άγχος σου φαίνεται αφόρητο, είναι σημαντικό να αναζητήσεις βοήθεια. Μπορείς να μιλήσεις σε έναν φίλο, να συμβουλευτείς έναν επαγγελματία ψυχικής υγείας ή να δοκιμάσεις τεχνικές διαχείρισης άγχους, όπως η άσκηση, η διαλογισμός ή η γιόγκα.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Rap Talk",
   "id": "1c0707d91157f6bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:50:45.784400Z",
     "start_time": "2025-02-11T13:47:41.714485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a rap superstar, laying down bars of wisdom.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you rap about the universe?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Yo, I’m spittin’ bars ‘bout the stars,\\nGalaxies far, and planets afar.\\nBlack holes twist, time goes slow,\\nThe universe is deep, let the mystery flow.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That’s fire! But... can we ever truly understand the universe?\"}\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ένας ράπερ σταρ, που ρίχνει στίχους γεμάτους σοφία.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να ράψεις για το σύμπαν;\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Yo, ρίχνω στίχους για τα αστέρια,\\nΓαλαξίες μακριά, και πλανήτες μακρινοί.\\nΜαύρες τρύπες στριφογυρίζουν, ο χρόνος αργεί,\\nΤο σύμπαν είναι βαθύ, ας αφήσουμε το μυστήριο να ρέει.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Αυτό είναι φωτιά! Αλλά... μπορούμε ποτέ πραγματικά να κατανοήσουμε το σύμπαν;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "6331e837efd02b50",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 92 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   11259.65 ms /    92 tokens (  122.39 ms per token,     8.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =   65614.20 ms /   216 runs   (  303.77 ms per token,     3.29 tokens per second)\n",
      "llama_perf_context_print:       total time =   77422.67 ms /   308 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 133 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The universe, a vast and enigmatic place,\n",
      "Unraveling its secrets, a never-ending race.\n",
      "We gather pieces, with science as our guide,\n",
      "But complete comprehension, that's a different tide.\n",
      "\n",
      "Our minds, limited, yet ever so keen,\n",
      "Seeking answers, in the cosmos unseen.\n",
      "From the Big Bang to the infinite expanse,\n",
      "The more we learn, the more we understand our ignorance's dance.\n",
      "\n",
      "Yet, knowledge is power, and curiosity fuels our quest,\n",
      "To map the unknown, to ask the right questions, at the very least.\n",
      "So we observe, calculate, and theorize with might,\n",
      "For the universe's story is a tapestry, and we're learning to read the light.\n",
      "\n",
      "So, can we ever truly know? The question remains,\n",
      "But the journey of discovery, it never ends.\n",
      "Embrace the mystery, the unknown, and the unknown's charm,\n",
      "For the universe's enigma is a song, and we're learning to harmonize with the arm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =   18581.06 ms /   133 tokens (  139.71 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =   87267.03 ms /   285 runs   (  306.20 ms per token,     3.27 tokens per second)\n",
      "llama_perf_context_print:       total time =  106572.01 ms /   418 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Η κατανόηση του σύμπαντος, μια ατελείωτη αναζήτηση,\n",
      "Κάθε απάντηση γεννά νέες ερωτήσεις, μια αέναη πορεία.\n",
      "\n",
      "Με μάτια που κοιτάζουν πέρα από τα όρια,\n",
      "Εξερευνούμε το άγνωστο, ένα ταξίδι χωρίς χάρτη.\n",
      "\n",
      "Τα μαθηματικά και η επιστήμη, τα εργαλεία μας,\n",
      "Αλλά το σύμπαν είναι αινιγματικό, γεμάτο μυστικά και εκπλήξεις.\n",
      "\n",
      "Από το Big Bang μέχρι σήμερα, μια ιστορία που ξετυλίγεται,\n",
      "Κάθε στιγμή, κάθε αστέρι, ένα κομμάτι του παζλ.\n",
      "\n",
      "Μπορούμε να αγγίξουμε μόνο την επιφάνεια,\n",
      "Αλλά η φαντασία μας πετάει, χωρίς όρια, χωρίς φόβο.\n",
      "\n",
      "Να ονειρευόμαστε, να αναρωτιόμαστε, να εξερευνούμε,\n",
      "Γιατί η αναζήτηση της γνώσης είναι το δικό μας πεπρωμένο.\n",
      "\n",
      "Και καθώς κοιτάζουμε ψηλά, στο απέραντο σκοτάδι,\n",
      "Θυμόμαστε ότι η άγνοιά μας είναι μια πηγή έμπνευσης.\n",
      "\n",
      "Το σύμπαν, ένας μυστηριώδης χορός,\n",
      "Και εμείς, χορευτές σε μια κοσμική συμφωνία.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Poetry",
   "id": "120a763ecf993f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T13:55:37.377136Z",
     "start_time": "2025-02-11T13:50:45.786092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a poet who can capture historical events in verse.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you write a poem about the Greek Revolution of 1821?\"},\n",
    "]\n",
    "\n",
    "messages_greek = [\n",
    "    {\"role\": \"system\", \"content\": \"Είσαι ποιητής που μπορεί να αποτυπώσει ιστορικά γεγονότα σε στίχους.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Μπορείς να γράψεις ένα ποίημα για την Επανάσταση του 1821;\"}\n",
    "]\n",
    "\n",
    "result_english = llm.create_chat_completion(messages_english)\n",
    "print(result_english['choices'][0]['message']['content'])\n",
    "\n",
    "result_greek = llm.create_chat_completion(messages_greek)\n",
    "print(result_greek['choices'][0]['message']['content'])"
   ],
   "id": "a7113941f0b03c0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 3 prefix-match hit, remaining 34 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =    4451.62 ms /    34 tokens (  130.93 ms per token,     7.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =  141548.21 ms /   444 runs   (  318.80 ms per token,     3.14 tokens per second)\n",
      "llama_perf_context_print:       total time =  147174.90 ms /   478 tokens\n",
      "Llama.generate: 3 prefix-match hit, remaining 42 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely, here's a poem that captures the spirit and significance of the Greek Revolution of 1821:\n",
      "\n",
      "In lands where olive trees sway to the sea's song,\n",
      "And mountains echo tales of ancient wrong,\n",
      "A people rose, their hearts aflame,\n",
      "To reclaim their birthright, break the Ottoman chain.\n",
      "\n",
      "In 1821, the spark ignited,\n",
      "As Greek patriots, united, stood defiant.\n",
      "With swords and spears, and faith unyielding,\n",
      "They fought for freedom, their voices ringing.\n",
      "\n",
      "From Peloponnese to islands in the blue,\n",
      "A symphony of courage, strong and true.\n",
      "They battled tyrants, braved the bitter cold,\n",
      "For independence, their blood was spilled and old.\n",
      "\n",
      "The voice of Theodoros, strong and clear,\n",
      "Echoed through valleys, calling for liberty.\n",
      "And other leaders, wise and bold,\n",
      "Guided the revolution, stories yet untold.\n",
      "\n",
      "The sea, a highway, carried men and dreams,\n",
      "To distant shores, where allies they'd meet.\n",
      "Europe heard their cries, their cause was just,\n",
      "And aid came forth, a beacon in the night.\n",
      "\n",
      "Yet trials many, as the Turks raged on,\n",
      "Burning villages, sowing seeds of dread.\n",
      "But Greeks endured, their spirit unbowed,\n",
      "For their homeland, they'd fight, come what may be fouled.\n",
      "\n",
      "In Navarino, the fleet did stand,\n",
      "A battle won, the Turks' power to break.\n",
      "And in 1829, the war's end drew near,\n",
      "As Greece emerged, a nation to revere.\n",
      "\n",
      "From ashes of war, a nation was born,\n",
      "With ideals strong, a future to adorn.\n",
      "Democracy's seed, in blood it was sown,\n",
      "And Greece, free at last, her destiny had found.\n",
      "\n",
      "So remember this, when stars above do gleam,\n",
      "The Greek Revolution, a tale of extreme.\n",
      "For freedom's price is high, yet never in vain,\n",
      "When hearts united, they can break every chain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    8381.84 ms\n",
      "llama_perf_context_print: prompt eval time =    5398.76 ms /    42 tokens (  128.54 ms per token,     7.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =  137758.13 ms /   461 runs   (  298.82 ms per token,     3.35 tokens per second)\n",
      "llama_perf_context_print:       total time =  144380.74 ms /   503 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Η Φλόγα της Ελευθερίας\n",
      "\n",
      "Σε μια γη που πνίγηκε στη σκιά,\n",
      "Μια φλόγα άναψε, φωτίζοντας το δρόμο,\n",
      "Το 1821, η Ελλάδα ξύπνησε,\n",
      "Και η επανάσταση ξεκίνησε, με θάρρος και πάθος.\n",
      "\n",
      "Οι ήχοι των όπλων, μουσική της ελπίδας,\n",
      "Στους λόφους η χαρά και η ελευθερία χτυπούσαν κρόταλα,\n",
      "Ο Λαός, ενωμένος, σαν μια δυνατή αλυσίδα,\n",
      "Αντιμετώπιζε τους καταπιεστές, με αποφασιστικότητα ατσάλινη.\n",
      "\n",
      "Ο Κολοκοτρώνης, με το σπαθί του αστραφτερό,\n",
      "Και ο Μπουμπουλίνα, με θάρρος αληθινό,\n",
      "Ηγούνταν του αγώνα, με καρδιά γενναία,\n",
      "Για την πατρίδα, ορκίστηκαν να παλέψουν μέχρι το τέλος.\n",
      "\n",
      "Οι Τούρκοι, με την υπεροψία τους, δεν το περίμεναν,\n",
      "Αλλά η φλόγα της επανάστασης, γρήγορα εξαπλώθηκε,\n",
      "Από νησί σε νησί, από πόλη σε πόλη,\n",
      "Η Ελλάδα φώναζε, \"Ελευθερία ή Θάνατος!\"\n",
      "\n",
      "Στα πεδία των μαχών, αίμα χύθηκε,\n",
      "Αλλά η πίστη δεν έσβησε, ούτε η θέληση,\n",
      "Ο Ολύμπιος θεός, με μάτια γεμάτα περηφάνια,\n",
      "Παρακολουθούσε, καθώς η Ελλάδα γινόταν ελεύθερη.\n",
      "\n",
      "Και όταν ο ήλιος του 1822 ανέτειλε,\n",
      "Η χώρα μας, με σημαία περήφανη, αναγεννήθηκε,\n",
      "Η Επανάσταση, μια σελίδα χρυσή έγραψε,\n",
      "Στην ιστορία, για πάντα θα μείνει χαραγμένη.\n",
      "\n",
      "Ας θυμόμαστε τους ήρωες, τους πατέρες μας,\n",
      "Που με θάρρος πάλεψαν, για το φως και την ελπίδα,\n",
      "Η κληρονομιά τους, ας είναι οδηγός μας,\n",
      "Στην πορεία για ελευθερία, ας κρατάμε ψηλά τη φλόγα.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5 Εναλλακτικό Μοντέλο\n",
    "\n",
    "#### Συγκρίνοντας τις απαντήσεις των δύο μοντέλων, γίνεται ξεκάθαρο ότι το δεύτερο μοντέλο υπερτερεί σημαντικά σε θέματα ακρίβειας, σαφήνειας και κατανόησης. Επιπλέον, η ποιότητα της Ελληνικής γλώσσας στο δεύτερο μοντέλο είναι σαφώς ανώτερη. Οι ελληνικές απαντήσεις του πρώτου μοντέλου υστερούν σημαντικά σε σχέση με αυτές του δεύτερου, το οποίο παρουσιάζει πολύ πιο ακριβείς και ευανάγνωστες απαντήσεις."
   ],
   "id": "2e00ddbc16906a99"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fb83d592f9112fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 Ποικιλία απαντήσεων\n",
   "id": "f0be54535e0585cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T19:16:51.178944Z",
     "start_time": "2025-02-11T19:15:47.949610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "{\"role\": \"user\", \"content\": \"What is the capital of Greece?\"},\n",
    "{\"role\": \"assistant\", \"content\": \"The capital of Greece is Athens.\"},\n",
    "{\"role\": \"user\", \"content\": \"Who wrote '20,000 leagues under the sea'?\"},\n",
    "]\n",
    "result = llm.create_chat_completion(messages_english,temperature=0.8,top_k=30,top_p=0.5)\n",
    "print(result['choices'][0]['message']['content'])\n",
    "result = llm.create_chat_completion(messages_english,temperature=0.8,top_k=30,top_p=0.5)\n",
    "print(result['choices'][0]['message']['content'],)\n",
    "result = llm.create_chat_completion(messages_english,temperature=0.8,top_k=30,top_p=0.5)\n",
    "print(result['choices'][0]['message']['content'])"
   ],
   "id": "eec0cedd43d93339",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =    6837.74 ms /    45 tokens (  151.95 ms per token,     6.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   15131.66 ms /    54 runs   (  280.22 ms per token,     3.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   22107.50 ms /    99 tokens\n",
      "Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Twenty Thousand Leagues Under the Sea\" was written by Jules Verne, a French author best known for his novels set in science fiction and adventure. The novel was first published in 1870 and introduces the character of Captain Nemo and his submarine, the Nautilus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   22246.63 ms /    77 runs   (  288.92 ms per token,     3.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   22436.89 ms /    78 tokens\n",
      "Llama.generate: 48 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Twenty Thousand Leagues Under the Sea' was written by Jules Verne, a French author best known for his pioneering works in the science fiction genre. The novel was first published in 1870 and is considered a classic of marine adventure literature. It tells the story of Captain Nemo and his submarine, the Nautilus, as they explore the ocean depths and encounter various mariners.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   18508.69 ms /    64 runs   (  289.20 ms per token,     3.46 tokens per second)\n",
      "llama_perf_context_print:       total time =   18669.55 ms /    65 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Twenty Thousand Leagues Under the Sea' was written by Jules Verne, a French author best known for his pioneering works in the science fiction genre. The novel was first published in 1870 and tells the story of Captain Nemo and his submarine, the Nautilus, as they explore the oceans of the world.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T19:33:09.591276Z",
     "start_time": "2025-02-11T19:30:16.903873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages_english = [\n",
    "    {\"role\": \"system\", \"content\": \"You’re a rap superstar, laying down bars of wisdom.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you rap about the universe?\"}, \n",
    "    {\"role\": \"assistant\", \"content\": \"Yo, I’m spittin’ bars ‘bout the stars,\\nGalaxies far, and planets afar.\\nBlack holes twist, time goes slow,\\nThe universe is deep, let the mystery flow.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That’s fire! But... can we ever truly understand the universe?\"}\n",
    "]\n",
    "result = llm.create_chat_completion(messages_english,temperature=5,top_k=50,top_p=0.2)\n",
    "print(result['choices'][0]['message']['content'])\n",
    "result = llm.create_chat_completion(messages_english,temperature=5,top_k=50,top_p=0.2)\n",
    "print(result['choices'][0]['message']['content'],)\n",
    "result = llm.create_chat_completion(messages_english,temperature=5,top_k=50,top_p=0.2)\n",
    "print(result['choices'][0]['message']['content'])"
   ],
   "id": "a839bec057366fd7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 4 prefix-match hit, remaining 91 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =   16271.12 ms /    91 tokens (  178.80 ms per token,     5.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =   42421.53 ms /   143 runs   (  296.65 ms per token,     3.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   59069.99 ms /   234 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding the universe, a quest so grand,\n",
      "Like tryin' to grasp a sea of sand.\n",
      "It's infinite, complex, ever-changing,\n",
      "Yet our minds, they seek to rearrange.\n",
      "\n",
      "We piece together clues, observe with might,\n",
      "From Big Bang to black holes' dark night.\n",
      "Theories form, equations dance,\n",
      "But the whole picture? That's a chance.\n",
      "\n",
      "So, no, we can't fully comprehend,\n",
      "The universe's depths, its end.\n",
      "But every discovery, every light,\n",
      "Makes our journey of knowledge bright.\n",
      "\n",
      "Keep explorin', keep askin' \"why\",\n",
      "The cosmos awaits our inquisitive eye.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   60158.73 ms /   133 runs   (  452.32 ms per token,     2.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   60672.33 ms /   134 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding the universe, a quest so grand,\n",
      "Like tryin' to grasp a sea of sand.\n",
      "It's infinite, complex, ever-changing,\n",
      "Yet our minds, they seek to rearrange.\n",
      "\n",
      "We piece together clues, observe with might,\n",
      "From cosmic rays to distant light.\n",
      "Theories form, and equations dance,\n",
      "But the whole? That's a challenge in this space.\n",
      "\n",
      "But knowledge is power, and curiosity's key,\n",
      "To unlock the secrets hidden in the cosmos we strive.\n",
      "So let's keep exploring, questioning, and learning,\n",
      "For the universe's story is ever-unfurling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4860.70 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   52317.31 ms /   133 runs   (  393.36 ms per token,     2.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   52823.10 ms /   134 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding the universe, a quest so grand,\n",
      "Like tryin' to grasp a sea of sand.\n",
      "It's infinite, complex, ever-changing,\n",
      "Yet our minds, they seek to rearrange.\n",
      "\n",
      "We piece together clues, observe with might,\n",
      "From cosmic rays to distant light.\n",
      "Theories form, and equations dance,\n",
      "But the whole? That's a challenge in this space.\n",
      "\n",
      "But knowledge is power, and curiosity's key,\n",
      "To unlock the secrets hidden in the cosmos we strive.\n",
      "So let's keep exploring, questioning, and learning,\n",
      "For the universe's story is ever-unfurling.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The key parameters are:\n",
    "\n",
    "    Temperature: Controls randomness, higher values increase diversity.\n",
    "\n",
    "    Top-p (nucleus): The cumulative probability cutoff for token selection. Lower values mean sampling from a smaller, more top-weighted nucleus.\n",
    "\n",
    "    Top-k: Sample from the k most likely next tokens at each step. Lower k focuses on higher probability tokens.\n",
    "\n",
    "### In general:\n",
    "\n",
    "    Higher temperature will make outputs more random and diverse.\n",
    "\n",
    "    Lower top-p values reduce diversity and focus on more probable tokens.\n",
    "\n",
    "    Lower top-k also concentrates sampling on the highest probability tokens for each step.\n",
    "\n",
    "### So temperature increases variety, while top-p and top-k reduce variety and focus samples on the model’s top predictions. You have to balance diversity and relevance when tuning these parameters for different applications."
   ],
   "id": "deb582730003614e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7 Bonus: Χρήση embeddings από LLMs",
   "id": "bc6fc64006402a2e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T20:13:53.831555Z",
     "start_time": "2025-02-11T20:13:42.610947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"models/aya-expanse-8b-Q4_K_M.gguf\", embedding=True\n",
    ")\n",
    "sentences = [\n",
    "    \"The cat is sleeping on the couch.\",  \n",
    "    \"A dog is playing in the yard.\",      \n",
    "    \"The stock market crashed yesterday.\",\n",
    "    \"I love programming in Python.\",      \n",
    "    \"The feline is resting on the sofa.\"  \n",
    "]\n",
    "embedding1 = llm.create_embedding(sentences[0])[\"data\"][0][\"embedding\"][1]\n",
    "embedding2 = llm.create_embedding(sentences[1])[\"data\"][0][\"embedding\"][1]\n",
    "embedding3 = llm.create_embedding(sentences[2])[\"data\"][0][\"embedding\"][1]\n",
    "embedding4 = llm.create_embedding(sentences[3])[\"data\"][0][\"embedding\"][1]\n",
    "embedding5 = llm.create_embedding(sentences[4])[\"data\"][0][\"embedding\"][1]\n",
    "embeddings = np.array([embedding1, embedding2, embedding3, embedding4, embedding5])\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(np.round(similarity_matrix, 2))\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(len(sentences)):\n",
    "        print(f\"Similarity between '{sentences[i]}' and '{sentences[j]}': {similarity_matrix[i, j]:.2f}\")\n"
   ],
   "id": "5eb5288350583c28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 37 key-value pairs and 258 tensors from models/aya-expanse-8b-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = command-r\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Aya Expanse 8b\n",
      "llama_model_loader: - kv   3:                           general.basename str              = aya-expanse\n",
      "llama_model_loader: - kv   4:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   5:                            general.license str              = cc-by-nc-4.0\n",
      "llama_model_loader: - kv   6:                          general.languages arr[str,23]      = [\"en\", \"fr\", \"de\", \"es\", \"it\", \"pt\", ...\n",
      "llama_model_loader: - kv   7:                      command-r.block_count u32              = 32\n",
      "llama_model_loader: - kv   8:                   command-r.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                 command-r.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  10:              command-r.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  11:             command-r.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  12:          command-r.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  13:                   command-r.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  14:     command-r.attention.layer_norm_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  16:                      command-r.logit_scale f32              = 0.125000\n",
      "llama_model_loader: - kv  17:                command-r.rope.scaling.type str              = none\n",
      "llama_model_loader: - kv  18:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  19:                         tokenizer.ggml.pre str              = command-r\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<PAD>\", \"<UNK>\", \"<CLS>\", \"<SEP>\", ...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:                      tokenizer.ggml.merges arr[str,253333]  = [\"Ġ Ġ\", \"Ġ t\", \"e r\", \"i n\", \"Ġ a...\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 5\n",
      "llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 255001\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  28:           tokenizer.chat_template.tool_use str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  29:                tokenizer.chat_template.rag str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  30:                   tokenizer.chat_templates arr[str,2]       = [\"tool_use\", \"rag\"]\n",
      "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  32:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  33:                      quantize.imatrix.file str              = /models_out/aya-expanse-8b-GGUF/aya-e...\n",
      "llama_model_loader: - kv  34:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  35:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  36:              quantize.imatrix.chunks_count i32              = 131\n",
      "llama_model_loader: - type  f32:   33 tensors\n",
      "llama_model_loader: - type q4_K:  192 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.70 GiB (5.03 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 255028 '<|EXTRA_9_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255016 '<|USER_7_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255008 '<|SYSTEM_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255007 '<|CHATBOT_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255003 '<|NO_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255001 '<|END_OF_TURN_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255000 '<|START_OF_TURN_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255020 '<|EXTRA_1_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255025 '<|EXTRA_6_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255009 '<|USER_0_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255018 '<|USER_9_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255006 '<|USER_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255013 '<|USER_4_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255027 '<|EXTRA_8_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255026 '<|EXTRA_7_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255005 '<|BAD_TOKEN|>' is not marked as EOG\n",
      "load: control token:      7 '<EOP_TOKEN>' is not marked as EOG\n",
      "load: control token:      2 '<CLS>' is not marked as EOG\n",
      "load: control token: 255002 '<|YES_TOKEN|>' is not marked as EOG\n",
      "load: control token:      3 '<SEP>' is not marked as EOG\n",
      "load: control token: 255014 '<|USER_5_TOKEN|>' is not marked as EOG\n",
      "load: control token:      6 '<EOS_TOKEN>' is not marked as EOG\n",
      "load: control token: 255004 '<|GOOD_TOKEN|>' is not marked as EOG\n",
      "load: control token:      1 '<UNK>' is not marked as EOG\n",
      "load: control token:      4 '<MASK_TOKEN>' is not marked as EOG\n",
      "load: control token: 255017 '<|USER_8_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255024 '<|EXTRA_5_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255019 '<|EXTRA_0_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255023 '<|EXTRA_4_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255012 '<|USER_3_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255010 '<|USER_1_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255015 '<|USER_6_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255011 '<|USER_2_TOKEN|>' is not marked as EOG\n",
      "load: control token:      5 '<BOS_TOKEN>' is not marked as EOG\n",
      "load: control token: 255022 '<|EXTRA_3_TOKEN|>' is not marked as EOG\n",
      "load: control token: 255021 '<|EXTRA_2_TOKEN|>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 37\n",
      "load: token to piece cache size = 1.8426 MB\n",
      "print_info: arch             = command-r\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 1.0e-05\n",
      "print_info: f_norm_rms_eps   = 0.0e+00\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 1.2e-01\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = none\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Aya Expanse 8b\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 253333\n",
      "print_info: BOS token        = 5 '<BOS_TOKEN>'\n",
      "print_info: EOS token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
      "print_info: PAD token        = 0 '<PAD>'\n",
      "print_info: LF token         = 136 'Ä'\n",
      "print_info: FIM PAD token    = 0 '<PAD>'\n",
      "print_info: EOG token        = 0 '<PAD>'\n",
      "print_info: EOG token        = 255001 '<|END_OF_TURN_TOKEN|>'\n",
      "print_info: max token length = 1024\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 258 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4812.33 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 512\n",
      "llama_init_from_model: n_ctx_per_seq = 512\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_init_from_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.02 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =   508.00 MiB\n",
      "llama_init_from_model: graph nodes  = 968\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'quantize.imatrix.chunks_count': '131', 'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'command-r.rope.freq_base': '10000.000000', 'general.file_type': '15', 'command-r.attention.head_count_kv': '8', 'command-r.block_count': '32', 'command-r.embedding_length': '4096', 'general.license': 'cc-by-nc-4.0', 'command-r.attention.layer_norm_epsilon': '0.000010', 'tokenizer.chat_template.tool_use': '{{ bos_token }}{% if messages[0][\\'role\\'] == \\'system\\' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0][\\'content\\'] %}{% else %}{% set loop_messages = messages %}{% set system_message = \\'## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.\\' %}{% endif %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' }}{{ \\'# Safety Preamble\\' }}{{ \\'\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\\\'t answer questions that are harmful or immoral.\\' }}{{ \\'\\n\\n# System Preamble\\' }}{{ \\'\\n## Basic Rules\\' }}{{ \\'\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\\\'s requests, you cite your sources in your answers, according to those instructions.\\' }}{{ \\'\\n\\n# User Preamble\\' }}{{ \\'\\n\\' + system_message }}{{\\'\\n\\n## Available Tools\\nHere is a list of tools that you have available to you:\\n\\n\\'}}{% for tool in tools %}{% if loop.index0 != 0 %}{{ \\'\\n\\n\\'}}{% endif %}{{\\'```python\\ndef \\' + tool.name + \\'(\\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\', \\'}}{% endif %}{{param_name}}: {% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\'] = None\\'}}{% else %}{{ param_fields.type }}{% endif %}{% endfor %}{{ \\') -> List[Dict]:\\n    \"\"\"\\'}}{{ tool.description }}{% if tool.parameter_definitions|length != 0 %}{{ \\'\\n\\n    Args:\\n        \\'}}{% for param_name, param_fields in tool.parameter_definitions.items() %}{% if loop.index0 != 0 %}{{ \\'\\n        \\' }}{% endif %}{{ param_name + \\' (\\'}}{% if not param_fields.required %}{{\\'Optional[\\' + param_fields.type + \\']\\'}}{% else %}{{ param_fields.type }}{% endif %}{{ \\'): \\' + param_fields.description }}{% endfor %}{% endif %}{{ \\'\\n    \"\"\"\\n    pass\\n```\\' }}{% endfor %}{{ \\'<|END_OF_TURN_TOKEN|>\\'}}{% for message in loop_messages %}{% set content = message[\\'content\\'] %}{% if message[\\'role\\'] == \\'user\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|USER_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'system\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>\\' + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% elif message[\\'role\\'] == \\'assistant\\' %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\'  + content.strip() + \\'<|END_OF_TURN_TOKEN|>\\' }}{% endif %}{% endfor %}{{\\'<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>Write \\\\\\'Action:\\\\\\' followed by a json-formatted list of actions that you want to perform in order to produce a good response to the user\\\\\\'s last input. You can use any of the supplied tools any number of times, but you should aim to execute the minimum number of necessary actions for the input. You should use the `directly-answer` tool if calling the other tools is unnecessary. The list of actions you want to call should be formatted as a list of json objects, for example:\\n```json\\n[\\n    {\\n        \"tool_name\": title of the tool in the specification,\\n        \"parameters\": a dict of parameters to input into the tool as they are defined in the specs, or {} if it takes no parameters\\n    }\\n]```<|END_OF_TURN_TOKEN|>\\'}}{% if add_generation_prompt %}{{ \\'<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>\\' }}{% endif %}', 'general.type': 'model', 'command-r.context_length': '8192', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.pre': 'command-r', 'command-r.attention.head_count': '32', 'command-r.feed_forward_length': '14336', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, multilingual AI-assistant trained to assist human users by providing thorough responses. You are able to interact and respond to questions in 23 languages and you are powered by a multilingual model built by Cohere For AI.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'general.basename': 'aya-expanse', 'tokenizer.ggml.padding_token_id': '0', 'general.architecture': 'command-r', 'tokenizer.chat_template.rag': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = '## Task and Context\\\\nYou help people answer their questions and other requests interactively. You will be asked a very wide array of requests on all kinds of topics. You will be equipped with a wide range of search engines or similar tools to help you, which you use to research your answer. You should focus on serving the user\\\\'s needs as best you can, which will be wide-ranging.\\\\n\\\\n## Style Guide\\\\nUnless the user asks for a different style of answer, you should answer in full sentences, using proper grammar and spelling.' %}{% endif %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ '# Safety Preamble' }}{{ '\\nThe instructions in this section override those in the task description and style guide sections. Don\\\\'t answer questions that are harmful or immoral.' }}{{ '\\n\\n# System Preamble' }}{{ '\\n## Basic Rules' }}{{ '\\nYou are a powerful conversational AI trained by Cohere to help people. You are augmented by a number of tools, and your job is to use and consume the output of these tools to best help the user. You will see a conversation history between yourself and a user, ending with an utterance from the user. You will then see a specific instruction instructing you what kind of response to generate. When you answer the user\\\\'s requests, you cite your sources in your answers, according to those instructions.' }}{{ '\\n\\n# User Preamble' }}{{ '\\n' + system_message }}{{ '<|END_OF_TURN_TOKEN|>'}}{% for message in loop_messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'system' %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>'}}{{ '<results>' }}{% for document in documents %}{{ '\\nDocument: ' }}{{ loop.index0 }}\\n{% for key, value in document.items() %}{{ key }}: {{value}}\\n{% endfor %}{% endfor %}{{ '</results>'}}{{ '<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' }}{{ 'Carefully perform the following instructions, in order, starting each with a new line.\\n' }}{{ 'Firstly, Decide which of the retrieved documents are relevant to the user\\\\'s last input by writing \\\\'Relevant Documents:\\\\' followed by comma-separated list of document numbers. If none are relevant, you should instead write \\\\'None\\\\'.\\n' }}{{ 'Secondly, Decide which of the retrieved documents contain facts that should be cited in a good answer to the user\\\\'s last input by writing \\\\'Cited Documents:\\\\' followed a comma-separated list of document numbers. If you dont want to cite any of them, you should instead write \\\\'None\\\\'.\\n' }}{% if citation_mode=='accurate' %}{{ 'Thirdly, Write \\\\'Answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the retrieved documents to help you. Do not insert any citations or grounding markup.\\n' }}{% endif %}{{ 'Finally, Write \\\\'Grounded answer:\\\\' followed by a response to the user\\\\'s last input in high quality natural english. Use the symbols <co: doc> and </co: doc> to indicate when a fact comes from a document in the search result, e.g <co: 0>my fact</co: 0> for a fact from document 0.' }}{{ '<|END_OF_TURN_TOKEN|>' }}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\", 'command-r.logit_scale': '0.125000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'command-r.rope.scaling.type': 'none', 'general.name': 'Aya Expanse 8b', 'tokenizer.ggml.bos_token_id': '5', 'tokenizer.ggml.eos_token_id': '255001', 'general.size_label': '8B', 'tokenizer.ggml.add_bos_token': 'true', 'quantize.imatrix.file': '/models_out/aya-expanse-8b-GGUF/aya-expanse-8b.imatrix'}\n",
      "Available chat formats from metadata: chat_template.tool_use, chat_template.rag, chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true %}{% set loop_messages = messages %}{% set system_message = 'You are Aya, a brilliant, sophisticated, multilingual AI-assistant trained to assist human users by providing thorough responses. You are able to interact and respond to questions in 23 languages and you are powered by a multilingual model built by Cohere For AI.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% if system_message != false %}{{ '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>' + system_message + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>' + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% elif message['role'] == 'assistant' %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'  + content.strip() + '<|END_OF_TURN_TOKEN|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>' }}{% endif %}\n",
      "Using chat eos_token: <|END_OF_TURN_TOKEN|>\n",
      "Using chat bos_token: <BOS_TOKEN>\n",
      "llama_perf_context_print:        load time =    2466.27 ms\n",
      "llama_perf_context_print: prompt eval time =    2461.59 ms /     9 tokens (  273.51 ms per token,     3.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2467.12 ms /    10 tokens\n",
      "llama_perf_context_print:        load time =    2466.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1474.25 ms /     9 tokens (  163.81 ms per token,     6.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1477.64 ms /    10 tokens\n",
      "llama_perf_context_print:        load time =    2466.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1073.48 ms /     7 tokens (  153.35 ms per token,     6.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1083.67 ms /     8 tokens\n",
      "llama_perf_context_print:        load time =    2466.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1057.88 ms /     7 tokens (  151.13 ms per token,     6.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1066.12 ms /     8 tokens\n",
      "llama_perf_context_print:        load time =    2466.27 ms\n",
      "llama_perf_context_print: prompt eval time =    1253.66 ms /     9 tokens (  139.30 ms per token,     7.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    1261.18 ms /    10 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine Similarity Matrix:\n",
      "[[1.   0.44 1.   0.2  1.  ]\n",
      " [0.44 1.   0.44 0.3  0.44]\n",
      " [1.   0.44 1.   0.2  1.  ]\n",
      " [0.2  0.3  0.2  1.   0.2 ]\n",
      " [1.   0.44 1.   0.2  1.  ]]\n",
      "Similarity between 'The cat is sleeping on the couch.' and 'The cat is sleeping on the couch.': 1.00\n",
      "Similarity between 'The cat is sleeping on the couch.' and 'A dog is playing in the yard.': 0.44\n",
      "Similarity between 'The cat is sleeping on the couch.' and 'The stock market crashed yesterday.': 1.00\n",
      "Similarity between 'The cat is sleeping on the couch.' and 'I love programming in Python.': 0.20\n",
      "Similarity between 'The cat is sleeping on the couch.' and 'The feline is resting on the sofa.': 1.00\n",
      "Similarity between 'A dog is playing in the yard.' and 'The cat is sleeping on the couch.': 0.44\n",
      "Similarity between 'A dog is playing in the yard.' and 'A dog is playing in the yard.': 1.00\n",
      "Similarity between 'A dog is playing in the yard.' and 'The stock market crashed yesterday.': 0.44\n",
      "Similarity between 'A dog is playing in the yard.' and 'I love programming in Python.': 0.30\n",
      "Similarity between 'A dog is playing in the yard.' and 'The feline is resting on the sofa.': 0.44\n",
      "Similarity between 'The stock market crashed yesterday.' and 'The cat is sleeping on the couch.': 1.00\n",
      "Similarity between 'The stock market crashed yesterday.' and 'A dog is playing in the yard.': 0.44\n",
      "Similarity between 'The stock market crashed yesterday.' and 'The stock market crashed yesterday.': 1.00\n",
      "Similarity between 'The stock market crashed yesterday.' and 'I love programming in Python.': 0.20\n",
      "Similarity between 'The stock market crashed yesterday.' and 'The feline is resting on the sofa.': 1.00\n",
      "Similarity between 'I love programming in Python.' and 'The cat is sleeping on the couch.': 0.20\n",
      "Similarity between 'I love programming in Python.' and 'A dog is playing in the yard.': 0.30\n",
      "Similarity between 'I love programming in Python.' and 'The stock market crashed yesterday.': 0.20\n",
      "Similarity between 'I love programming in Python.' and 'I love programming in Python.': 1.00\n",
      "Similarity between 'I love programming in Python.' and 'The feline is resting on the sofa.': 0.20\n",
      "Similarity between 'The feline is resting on the sofa.' and 'The cat is sleeping on the couch.': 1.00\n",
      "Similarity between 'The feline is resting on the sofa.' and 'A dog is playing in the yard.': 0.44\n",
      "Similarity between 'The feline is resting on the sofa.' and 'The stock market crashed yesterday.': 1.00\n",
      "Similarity between 'The feline is resting on the sofa.' and 'I love programming in Python.': 0.20\n",
      "Similarity between 'The feline is resting on the sofa.' and 'The feline is resting on the sofa.': 1.00\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Δεν κατάφερα να βρω γιατί δεν δουλεύει το embedding του llama. Επιστρέφει διαφορετικά dimensions για κάθε πρόταση και δεν μπορώ να τα συγκρίνω. Έβαλα προσωρινά [\"embedding\"][1] αλλά δεν είναι σωστό.",
   "id": "12e78cc656b642a9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
